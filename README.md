# Website-to-VectorDB â€” **Selenium Edition** (Streamlit + OpenAI + Pinecone)

JavaScript-heavy sites? No problem. This app uses **Selenium** (headless Chrome) to render pages,
then extracts text, chunks it, embeds with OpenAI, and uploads to a **Pinecone Serverless** index.

---

## âœ¨ What You Get

- âœ… Selenium-based crawler (respects `robots.txt`, internal links only, polite delay)
- âœ… Emergency **STOP** button for crawling and uploading
- âœ… Char-based chunking (defaults to `1000` chars) to mirror your original logic
- âœ… Batched embeddings + Pinecone upserts with progress via Streamlit status UI
- âœ… Clean, modular project structure and env-driven config

---

## ðŸ§± Project Structure

```text
website-to-vectordb-selenium/
â”œâ”€ app.py                        # Streamlit UI (Selenium-driven pipeline)
â”œâ”€ src/
â”‚  â”œâ”€ config.py                  # API keys & settings
â”‚  â”œâ”€ crawler/
â”‚  â”‚  â”œâ”€ webdriver.py            # Headless Chrome WebDriver (cached)
â”‚  â”‚  â””â”€ selenium_crawler.py     # Polite Selenium crawler
â”‚  â”œâ”€ embeddings/
â”‚  â”‚  â”œâ”€ clients.py              # OpenAI & Pinecone clients
â”‚  â”‚  â””â”€ pipeline_selenium.py    # Char-chunk + embed + upsert
â”‚  â”œâ”€ ui/                        # (reserved)
â”‚  â””â”€ utils/                     # (reserved)
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â””â”€ README.md
```

---

## ðŸš€ Quickstart

**Prereqs**: Python 3.10+, **Google Chrome** installed on the host.

```bash
git clone <your-repo-url>.git
cd website-to-vectordb-selenium
cp .env.example .env  # add your keys
pip install -r requirements.txt
streamlit run app.py
```

### .env

| Variable | Description | Default |
| --- | --- | --- |
| `OPENAI_API_KEY` | Your OpenAI API key | â€” |
| `PINECONE_API_KEY` | Your Pinecone API key | â€” |
| `EMBEDDING_MODEL` | Embedding model | `text-embedding-3-small` |
| `EMBEDDING_DIMENSION` | Vector dims | `1536` |
| `EMBEDDING_BATCH_SIZE` | Upsert batch size | `100` |
| `DEFAULT_POLITE_DELAY_SECONDS` | Delay after load (sec) | `2.0` |
| `CRAWLER_USER_AGENT` | HTTP UA string | Chrome-like UA |
| `PAGE_LOAD_TIMEOUT` | Max load time (sec) | `20` |
| `JS_CHUNK_SIZE` | Char limit per chunk | `1000` |

---

## ðŸ§  How It Works

1. **Crawl (Selenium)**: Headless Chrome loads each page so dynamic content renders.
   Basic `robots.txt` rules are respected, and only **internal** links are followed.
2. **Chunk**: Text is split into ~`JS_CHUNK_SIZE`-sized char chunks (to match your original script).
3. **Embed**: OpenAI Embeddings API runs in **batches** with robust error messages.
4. **Upload**: Vectors are upserted to a Pinecone **serverless** index.

---

## ðŸ§ª Troubleshooting

- **Selenium/Chrome errors**: Ensure Google Chrome is installed and the machine has sufficient memory/CPU.
- **Keys missing**: The sidebar highlights which key is missing.
- **Permissions**: Always confirm you're allowed to crawl a domain, and respect each site's `robots.txt`.
- **Slow sites**: Increase `DEFAULT_POLITE_DELAY_SECONDS` in the sidebar or `.env`.

---

## ðŸ“„ License

MIT (or your preferred license).
